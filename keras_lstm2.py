# -*- coding: utf-8 -*-
"""Keras_LSTM2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oncB0csZ15-WgQjDcvPagM8vR8x6Ut3T

**Keras for Beginners: Implementing a Recurrent Neural Network**

https://victorzhou.com/blog/keras-rnn-tutorial/

**The Problem: Classifying Movie Reviews**
"""

!wget https://victorzhou.com/movie-reviews-dataset.zip

!unzip movie-reviews-dataset

import keras
from keras.models import *
from keras.layers import *
import cv2
import os
import numpy as np
import pandas as pd
#import keras as K
import tensorflow as tf
import matplotlib.pyplot as plt
from cv2 import resize
from tensorflow.keras.preprocessing import text_dataset_from_directory

DATASET_DIR='/content/movie-reviews-dataset/train'
os.listdir(DATASET_DIR)

# Assumes you're in the root level of the dataset directory.
# If you aren't, you'll need to change the relative paths here.
train_data = text_dataset_from_directory('/content/movie-reviews-dataset/train')
test_data = text_dataset_from_directory('/content/movie-reviews-dataset/test')

from tensorflow.keras.preprocessing import text_dataset_from_directory
from tensorflow.strings import regex_replace

def prepareData(dir):
  data = text_dataset_from_directory(dir)
  return data.map(
    lambda text, label: (regex_replace(text, '<br />', ' '), label),
  )

train_data = prepareData('/content/movie-reviews-dataset/train')
test_data = prepareData('/content/movie-reviews-dataset/test')

for text_batch, label_batch in train_data.take(1):
  print(text_batch.numpy()[0])
  print(label_batch.numpy()[0]) # 0 = negative, 1 = positive

"""3. Building the Model

"""

from tensorflow.keras.models import Sequential
from tensorflow.keras import Input

model = Sequential()
model.add(Input(shape=(1,), dtype="string"))

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

max_tokens = 1000
max_len = 100
vectorize_layer = TextVectorization(
  # Max vocab size. Any words outside of the max_tokens most common ones
  # will be treated the same way: as "out of vocabulary" (OOV) tokens.
  max_tokens=max_tokens,
  # Output integer indices, one per string token
  output_mode="int",
  # Always pad or truncate to exactly this many tokens
  output_sequence_length=max_len,
)

# Call adapt(), which fits the TextVectorization layer to our text dataset.
# This is when the max_tokens most common words (i.e. the vocabulary) are selected.
train_texts = train_data.map(lambda text, label: text)
vectorize_layer.adapt(train_texts)

from tensorflow.keras.layers import Embedding

# Previous layer: TextVectorization
max_tokens = 1000
# ...
model.add(vectorize_layer)

# Note that we're using max_tokens + 1 here, since there's an
# out-of-vocabulary (OOV) token that gets added to the vocab.
model.add(Embedding(max_tokens + 1, 128))

""" The Recurrent Laye"""

from tensorflow.keras.layers import LSTM

# 64 is the "units" parameter, which is the
# dimensionality of the output space.
model.add(LSTM(64))

from tensorflow.keras.layers import Dense

model.add(Dense(64, activation="relu"))
model.add(Dense(1, activation="sigmoid"))

"""Compiling and training the Model"""

model.compile(
  optimizer='adam',
  loss='binary_crossentropy',
  metrics=['accuracy'],
)

model.fit(train_data, epochs=10)

# Should print a very high score like 0.98.
print(model.predict([
  "i loved it! highly recommend it to anyone and everyone looking for a great movie to watch.",
]))

# Should print a very low score like 0.01.
print(model.predict([
  "this was awful! i hated it so much, nobody should watch this. the acting was terrible, the music was terrible, overall it was just bad.",
]))

print(model.predict(["رثقغ لاشي",]))
print(model.predict([" verall it was just bad",]))
print(model.predict([" verall it was just good",]))

